{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Minilab","text":""},{"location":"#introduction","title":"Introduction","text":"<p>La gestion efficace des appareils \u00e0 la p\u00e9riph\u00e9rie (edge) des r\u00e9seaux informatiques est devenue essentielle. Red Hat Device Edge \u00e9merge comme une solution polyvalente pour r\u00e9pondre aux d\u00e9fis sp\u00e9cifiques pos\u00e9s par la gestion des charges de travail sur des appareils \u00e0 la limite du r\u00e9seau. Cette plateforme offre une approche novatrice pour soutenir des t\u00e2ches vari\u00e9es sur des dispositifs aux ressources limit\u00e9es, rencontrant des contraintes de puissance, de refroidissement et de connectivit\u00e9.</p>"},{"location":"#quest-ce-que-red-hat-device-edge","title":"Qu'est-ce que Red Hat Device Edge ?","text":"<p>Red Hat Device Edge est une plateforme flexible con\u00e7ue pour r\u00e9pondre aux besoins des dispositifs \u00e0 la p\u00e9riph\u00e9rie, tels que les outils de ligne d'assemblage, les passerelles IoT, les points de vente et les contr\u00f4leurs industriels. Ces appareils op\u00e8rent souvent dans des environnements o\u00f9 les ressources informatiques sont limit\u00e9es et o\u00f9 l'acc\u00e8s peut \u00eatre restreint. Red Hat Device Edge offre une approche uniforme et adaptable pour prendre en charge diverses charges de travail sur ces appareils, malgr\u00e9 leurs contraintes en termes de ressources.</p> <p></p>"},{"location":"add-setup/01_openvpn/","title":"Installation et configuration d'un VPN sur Scaleway","text":""},{"location":"add-setup/01_openvpn/#configuration-de-linstance","title":"Configuration de l'instance","text":"<p>Pour commencer, cliquez sur Compute &gt; Instances &gt; Create Instance.</p> <p>Completez le formulaire avec les valeurs ci-dessous.</p> Settings Value Availability Zone Paris 1 Select an Instance Dev1-S Choose an image Fedora <p>Ajoutez ensuite votre cl\u00e9 ssh et cliquez sur Cr\u00e9er une instance.</p>"},{"location":"add-setup/01_openvpn/#configuration-dopenvpn","title":"Configuration d'openvpn","text":"<p>Connectez-vous \u00e0 l'instance nouvellement cr\u00e9\u00e9e.</p> <p>R\u00e9cup\u00e9rez le script ci-dessous et rendez-le ex\u00e9cutable\u00a0:</p> <pre><code>curl -O https://raw.githubusercontent.com/angristan/openvpn-install/master/openvpn-install.sh\nchmod +x openvpn-install.sh\n</code></pre> <p>Puis \u00e9xecutez le</p> <pre><code>bash openvpn-install.sh\n</code></pre> <p>R\u00e9pondez aux diff\u00e9rentes questions du script et r\u00e9cup\u00e9rer le fichier <code>.ovpn</code>.</p> <p>NOTE: Pour ajouter un client suppl\u00e9mentaire, ex\u00e9cutez \u00e0 nouveau le m\u00eame script et s\u00e9lectionnez ``Add a new user```</p> <pre><code>bash openvpn-install.sh\n</code></pre>"},{"location":"add-setup/01_openvpn/#configurer-le-client-sur-une-machine-rhelfedora","title":"Configurer le client sur une machine rhel/fedora","text":"<p>Sur votre machine cliquez sur Settings et rendez-vous dans la partie Network. Cliquez sur le <code>+</code> puis sur <code>Import from file</code> et importer le fichier openvpn.</p> <p></p> <p>Cliquez sur <code>Add</code> .</p> <p>Il vous suffit maintenant d'activer le toggle pour se connecter au VPN.</p>"},{"location":"add-setup/02_keycloak/","title":"Installation de keycloack sur Scaleway","text":""},{"location":"add-setup/02_keycloak/#configuration-de-linstance","title":"Configuration de l'instance","text":"<p>Pour commencer, cliquez sur Compute &gt; Instances &gt; Create Instance.</p> <p>Completez le formulaire avec les valeurs ci-dessous.</p> Settings Value Availability Zone Paris 1 Select an Instance Dev1-S Choose an image Fedora <p>Ajoutez ensuite votre cl\u00e9 ssh et cliquez sur Cr\u00e9er une instance.</p>"},{"location":"add-setup/02_keycloak/#installation-de-keycloak","title":"Installation de Keycloak","text":"<p>Connectez-vous \u00e0 votre instance en utilisant ssh et suivez les instructions ci-dessous.</p> <p>Clonez le repo keycloack container pour installer le repo en mode podman.</p> <pre><code>git clone git@github.com:feven-NIT/keycloak-container.git\n</code></pre> <p>Rendez vous dans le Container file de keycloak</p> <pre><code>vi  keycloak-container/keycloak/Containerfile\n</code></pre> <p>Remplacez les lignes conrespondant aux variables d'environnement <code>KC_DB_URL</code> et <code>KC_HOSTNAME</code> par l'addresse IP ou le hostname de votre machine.</p> <p>Example :</p> <pre><code>FROM quay.io/keycloak/keycloak:latest as builder\n#https://www.keycloak.org/server/containers\n\n# Enable health and metrics support\nENV KC_HEALTH_ENABLED=true\nENV KC_METRICS_ENABLED=true\n\n# Configure a database vendor\nENV KC_DB=postgres\n\nWORKDIR /opt/keycloak\n# for demonstration purposes only, please make sure to use proper certificates in production instead\nRUN keytool -genkeypair -storepass password -storetype PKCS12 -keyalg RSA -keysize 2048 -dname \"CN=server\" -alias server -ext \"SAN:c=DNS:localhost,IP:163.172.171.89\" -keystore conf/server.keystore\nRUN /opt/keycloak/bin/kc.sh build\n\nFROM quay.io/keycloak/keycloak:latest\nARG DB_USER\nARG DB_PASSWORD\nCOPY --from=builder /opt/keycloak/ /opt/keycloak/\n\n# change these values to point to a running postgres instance\nENV KC_DB_URL=keycloak.cyber-ops.net:5432 ##TOUPDATE\nENV KC_DB_USERNAME=$DB_USER\nENV KC_DB_PASSWORD=$DB_PASSWORD\nENV KC_HOSTNAME=keycloak.cyber-ops.net ##TOUPDATE\nENTRYPOINT [\"/opt/keycloak/bin/kc.sh\"]\n</code></pre> <p>Then you can deploy keycloak</p> <pre><code>mkdir $HOME/postgres-keycloak-data\n./deploy.sh MY_DB_PASSWORD MY_KEYCLOAK_PASSWORD\n</code></pre>"},{"location":"add-setup/02_keycloak/#validate","title":"Validate","text":"<p>Go into your web browser.</p> <p>Search <code>https://keycloak.cyber-ops.net:8443</code></p> <p></p> <p>Click on Administration Console.</p> <p>In Username click on admin and put yout MY_KEYCLOAK_PASSWORD as password.</p> <p></p>"},{"location":"add-setup/03_install_wordpress/","title":"Installation de wordpress sur Scaleway","text":""},{"location":"add-setup/03_install_wordpress/#configuration-de-linstance","title":"Configuration de l'instance","text":"<p>Pour commencer, cliquez sur Compute &gt; Instances &gt; Create Instance.</p> <p>Completez le formulaire avec les valeurs ci-dessous.</p> Settings Value Availability Zone Paris 1 Select an Instance Dev1-S Choose an image Fedora <p>Ajoutez ensuite votre cl\u00e9 ssh et cliquez sur Cr\u00e9er une instance.</p>"},{"location":"add-setup/03_install_wordpress/#installation-dapache-et-de-php","title":"Installation d'Apache et de PHP","text":"<p>Connectez-vous \u00e0 votre instance en utilisant ssh et suivez les instructions ci-dessous.</p> <p>Tout d\u2019abord, installez le serveur Apache et php avec la commande suivante.</p> <pre><code>sudo dnf update -y\nsudo dnf install httpd -y\nsudo systemctl start httpd\nsudo systemctl enable httpd\nsudo dnf install php php-fpm php-mysqlnd php-opcache php-gd php-xml php-mbstring php-curl php-pecl-imagick php-pecl-zip libzip -y\n</code></pre> <p>Ensuite, \u00e9ditez le fichier de configuration PHP et modifiez les param\u00e8tres par d\u00e9faut adapt\u00e9s \u00e0 vos besoins\u00a0:</p> <pre><code>sudo vi /etc/php.ini\n</code></pre> <pre><code>max_execution_time = 300 \nmax_input_time = 300 \nmemory_limit = 512M \npost_max_size = 256M \nupload_max_filesize = 256M\n</code></pre> <p>Enregistrez et fermez le fichier, puis d\u00e9marrez et activez le service PHP-FPM avec la commande suivante.</p> <pre><code>sudo systemctl start php-fpm\nsudo systemctl enable php-fpm\n</code></pre>"},{"location":"add-setup/03_install_wordpress/#installer-et-configurer-la-base-de-donnees-mariadb","title":"Installer et configurer la base de donn\u00e9es MariaDB","text":"<p>Tout d\u2019abord, installez le serveur MariaDB avec la commande suivante.</p> <pre><code>sudo dnf install mariadb-server -y\nsudo systemctl start mariadb\nsudo systemctl enable mariadb\n</code></pre> <p>Ensuite, connectez-vous au shell MariaDB et cr\u00e9ez une base de donn\u00e9es et un utilisateur pour WordPress.</p> <p>NOTE: Update wpuser, localhost et password avec vos propres valeurs.</p> <pre><code>mysql\nMariaDB [(none)]&gt; CREATE DATABASE wpdb;\nMariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON wpdb.* TO 'wpuser'@'localhost' IDENTIFIED BY 'password';\nMariaDB [(none)]&gt; FLUSH PRIVILEGES;\nMariaDB [(none)]&gt; EXIT;\n</code></pre>"},{"location":"add-setup/03_install_wordpress/#telechargez-wordpress","title":"T\u00e9l\u00e9chargez WordPress","text":"<p>T\u00e9l\u00e9chargez la derni\u00e8re version de WordPress avec la commande suivante.</p> <pre><code>cd /var/www/html/\nwget https://www.wordpress.org/latest.tar.gz\ntar -xvf latest.tar.gz \ncd wordpress\ncp wp-config-sample.php wp-config.php\n</code></pre> <p>Ensuite, modifiez le fichier de configuration WordPress\u00a0:</p> <pre><code>sudo vi  wp-config.php\n</code></pre> <pre><code>/** The name of the database for WordPress */\ndefine( 'DB_NAME', 'wpdb' );\n\n/** Database username */\ndefine ('DB_USER', 'wpuser');\n\n/** Database password */\ndefine( 'DB_PASSWORD', 'password' );\n\n/** Database hostname */\ndefine( 'DB_HOST', 'localhost' );\n</code></pre> <p>Enregistrez et fermez le fichier, puis modifiez l'autorisation et la propri\u00e9t\u00e9 du r\u00e9pertoire WordPress.</p> <pre><code>chown -R apache:apache /var/www/html/wordpress\nchmod -R 755 /var/www/html/wordpress\n</code></pre> <p>Ensuite, vous devrez cr\u00e9er un h\u00f4te virtuel Apache pour d\u00e9finir votre r\u00e9pertoire et votre domaine WordPress.</p> <pre><code>vi /etc/httpd/conf.d/wp.conf\n</code></pre> <pre><code>&lt;VirtualHost *:80&gt;\n    ServerAdmin admin@example.com\n    ServerName wp.example.com\n    DocumentRoot /var/www/html/wordpress\n    &lt;Directory /var/www/html/wordpress&gt;\n        Allowoverride all\n    &lt;/Directory&gt;\n&lt;/VirtualHost&gt;\n</code></pre> <p>Enregistrez et fermez le fichier, puis red\u00e9marrez le service Apache pour appliquer les modifications.</p> <pre><code>systemctl restart httpd \n</code></pre>"},{"location":"add-setup/04_cyberark/","title":"Installation de conjur cyberark sur openshift","text":""},{"location":"add-setup/04_cyberark/#prerequisites","title":"Prerequisites","text":"<p>Installez le conjur cli. Avoir metal Lb ou un autres service de loadbalancing disponible.  NOTE: Il est surement possible de la faire via une route ,mais cela n'est pas trait\u00e9 dans ce guide.</p>"},{"location":"add-setup/04_cyberark/#installation-de-cyberark","title":"Installation de cyberark","text":"<p>Pour commencer nous allons installer conjur sur Openshift dans un namespace d\u00e9di\u00e9.</p> <pre><code>oc new-project florian-ns\n</code></pre> <p>Pour cela un Helm chart est mis \u00e0 disposition. Cloner le repo ci-dessous.</p> <pre><code>git clone TOCOMPLETE\n</code></pre> <p>Puis installez conjur </p> <pre><code>DATA_KEY=$(docker run --rm cyberark/conjur data-key generate)\nhelm install conjur ./conjur-oss --set dataKey=\"${DATA_KEY}\" --set authenticators=\"authn\\,authn-k8s/florian-ns\" --version 2.0.0 --set openshift.enabled=true --namespace florian-ns\n#Exemple \n#helm install conjur ./conjur-oss --set dataKey=\"8QYoky5p9szd9iXxSRmoOqGClOebfBPNCEpBl3Bn7jo=\" --set authenticators=\"authn\\,authn-k8s/conjur\" --version 2.0.0 --set openshift.enabled=true --namespace florian-ns\n</code></pre> <p>On configure ici 2 authenticators : <code>authn</code> pour une authentification en login/password standard et <code>authn-k8s</code> pour l'authentification des pods.</p> <p>On cr\u00e9er maintenant un compte par d\u00e9faut :</p> <pre><code>oc get po # On r\u00e9cupere ici le nom du pods\noc exec -it conjur-conjur-oss-6466669767-zbk6n -c conjur-oss -- conjurctl account create default\n</code></pre> <p>NOTE: Conserver l'ouput de la commande pr\u00e9c\u00e9dente. Le mot de passe sera utilis\u00e9 dans la suite de ce guide.</p> <p>Dans notre helm chart le certificat est configur\u00e9 pour le nom de domaine conjur.myorg.com. On va donc \u00e9diter le local base domain.</p> <pre><code>oc get svc conjur-conjur-oss-ingress -ojsonpath='{.status.loadBalancer.ingress[0].ip}'\n#10.253.234.69\n</code></pre> <pre><code>sudo vi /etc/hosts\n# add conjur.myorg.com 10.253.234.69\n</code></pre> <p>On va ensuite utiliser la commande <code>conjur init</code> pour cr\u00e9er un fichier de configuration (.conjurrc) qui contient les d\u00e9tails de connexion \u00e0 Conjur. Ce fichier se trouvera alors  sous le r\u00e9pertoire racine de l'utilisateur.</p> <pre><code>conjur init --url=https://conjur.myorg.com --account=default -s\n\n#The Conjur server's certificate SHA-1 fingerprint is:\n#XX:XX:XX\n\n#To verify this certificate, we recommend running the following command on the Conjur server:\n#openssl x509 -fingerprint -noout -in ~conjur/etc/ssl/conjur.pem\n\n#Trust this certificate? yes/no (Default: no): yes\n#File /home/youruser/conjur-server.pem exists. Overwrite? yes/no (Default: yes): yes\n#Certificate written to /home/feven/conjur-server.pem\n\n#File /home/youruser/.conjurrc exists. Overwrite? yes/no (Default: yes): yes\n#Configuration written to /home/youruser/.conjurrc\n\n#Successfully initialized the Conjur CLI\n#To start using the Conjur CLI, log in to the Conjur server by running `conjur login`\n</code></pre> <p>Vous pouvez maintenant vous connectez au serveur conjur. Le mot de passe est l'API key for admin g\u00e9n\u00e9r\u00e9 lors des \u00e9tapes pr\u00e9c\u00e9dentes.</p> <pre><code>conjur login\n#Enter your username: admin\n#Enter your password or API key (this will not be echoed):\n#WARNING:root:No supported keystore found! Saving credentials in plaintext in '/home/feven/.netrc'. #Make sure to logoff after you have finished using the CLI\n#Successfully logged in to Conjur\n</code></pre>"},{"location":"add-setup/04_cyberark/#gestion-des-token-pour-openshift","title":"Gestion des token pour Openshift","text":"<p>Pour automatiser la gestion des token conjur plusieurs m\u00e9thodes sont possibles. Dans ce guide nous allons ajouter \u00e0 notre pod un container qui s'authentifiera au serveur conjur \u00e0 l'aide du certificat g\u00e9n\u00e9r\u00e9 avec la commande <code>conjur init</code>. Ce token sera ensuite plac\u00e9 dans un dossier partager avec le container applicatif.</p> <p>Pour cela nous allons tout d'abord cr\u00e9er une configmap contenant le certificat.</p> <pre><code>oc create cm conjur-cert --from-file=ssl-certificate=/home/feven/conjur-server.pem\n</code></pre> <p>Nous allons maintenant cr\u00e9er une policy dans cyberark. Permettant au container TODO Partie \u00e0 developper. Explique ce que fait la policy.</p> <pre><code>vi policy-autom.yaml\n</code></pre> <pre><code>- !policy\n  id: conjur/authn-k8s/florian-ns\n  body:\n  - !webservice\n  - !layer\n    id: ubi8-app\n  - !host\n    id: florian-ns/service_account/florian-sa\n    annotations:\n      authn-k8s/namespace: florian-ns\n      authn-k8s/authentication-container-name: ubi-auth\n  - !grant\n    role: !layer ubi8-app\n    member: !host florian-ns/service_account/florian-sa\n  - !permit\n    resource: !webservice\n    privilege: [ authenticate ]\n    role: !layer ubi8-app\n# CA cert and key for creating client certificates\n  - !policy\n    id: ca\n    body:\n    - !variable\n      id: cert\n      annotations:\n        description: CA cert for Kubernetes Pods.\n    - !variable\n      id: key\n      annotations:\n        description: CA key for Kubernetes Pods.\n  - !variable test\n  - !permit\n    role: !layer ubi8-app\n    privileges: [ read, execute ]\n    resource: !variable test\n</code></pre> <pre><code>conjur policy load -b root -f policy.yaml\n</code></pre> <p>On peut maintenant ajouter une variable de test :</p> <pre><code> conjur variable set -i conjur/authn-k8s/florian-ns/test -v 1234567890\n</code></pre> <p>Ajouter maintenant les variable de clef/certificat.</p> <pre><code>conjur variable set -i conjur/authn-k8s/conjur/ca/cert -v \"$(cat /home/feven/cyberark/ca.key)\" \nconjur variable set -i conjur/authn-k8s/conjur/ca/cert -v \"$(cat /home/feven/conjur-server.pem)\" \n</code></pre> <p>Nous pouvons maintenant d\u00e9ployer notre pod constitu\u00e9 d'un pod applicatif et du conjur authenticator.</p> <pre><code>vi ubi8.yaml\n</code></pre> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubi8\n  namespace: florian-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubi8\n  template:\n    metadata:\n      labels:\n        app: ubi8\n    spec:\n      containers:\n      - name: ubi8\n        image: openshift.artifactory.mycloud.intranatixis.com/ubi8\n        command: [\"/bin/bash\", \"-c\", \"tail -f /dev/null\"]\n        env:\n        - name: CONJUR_APPLIANCE_URL\n          value: \"https://conjur-conjur-oss.florian-ns.svc.cluster.local/authn-k8s/florian-ns\"\n        - name: CONJUR_ACCOUNT\n          value: default\n        - name: CONJUR_AUTHN_TOKEN_FILE\n          value: /run/conjur/access-token\n        - name: CONJUR_SSL_CERTIFICATE\n          valueFrom:\n            configMapKeyRef:\n              name: conjur-cert\n              key: ssl-certificate\n        volumeMounts:\n        - mountPath: /run/conjur\n          name: conjur-access-token\n          readOnly: true\n      - name: ubi-auth\n        image: openshift.artifactory.mycloud.intranatixis.com/cyberark/conjur-openshift-authenticator\n        env:\n        - name: MY_POD_NAME\n          valueFrom:\n           fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n           fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n             fieldPath: status.podIP\n        - name: CONJUR_AUTHN_URL\n          value: \"https://conjur-conjur-oss.florian-ns.svc.cluster.local/authn-k8s/florian-ns\"\n        - name: CONJUR_ACCOUNT\n          value: default\n        - name: CONJUR_AUTHN_LOGIN\n          value: \"host/conjur/authn-k8s/florian-ns/florian-ns/service_account/florian-sa\"\n        - name: CONJUR_SSL_CERTIFICATE\n          valueFrom:\n            configMapKeyRef:\n              name: conjur-cert\n              key: ssl-certificate\n        volumeMounts:\n        - mountPath: /run/conjur\n          name: conjur-access-token\n      serviceAccountName: florian-sa\n      volumes:\n      - name: conjur-access-token\n        emptyDir:\n          medium: Memory\n</code></pre>"},{"location":"add-setup/04_cyberark/#validation","title":"Validation","text":"<p>Nous pouvons maintenant tester notre serveur conjur.</p> <p>Pour cela connectez vous au pod ubi8</p> <pre><code>oc rsh ubi8-7967957478-tc96s\n\nCONT_SESSION_TOKEN=$(cat /run/conjur/access-token| base64 | tr -d '\\r\\n')\ncurl -s -k -H \"Content-Type: application/json\" -H \"Authorization: Token token=\\\"$CONT_SESSION_TOKEN\\\"\" https://conjur-conjur-oss-ingress.florian-ns.svc.cluster.local/secrets/default/variable/conjur%2Fauthn-k8s%2Fflorian-ns%2Ftest\n1234567890\n</code></pre>"},{"location":"add-setup/05_kubeburner/","title":"Using kube-burner on OpenShift","text":""},{"location":"add-setup/05_kubeburner/#introduction","title":"Introduction","text":"<p>kube-burner is an open-source command-line tool designed for the creation and destruction of Kubernetes clusters and resources at scale. It simplifies the management and deployment of complex Kubernetes environments, making it an essential tool for DevOps engineers, SysAdmins, and those involved in continuous integration and delivery pipelines. In this tutorial, we'll learn how to use kube-burner to deploy a simple benchmark application on Red Hat OpenShift.</p>"},{"location":"add-setup/05_kubeburner/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ol> <li>A running instance of an OpenShift cluster.</li> <li>Kubernetes CLI (kubectl) installed and configured to communicate with your OpenShift cluster.</li> <li>Go programming language environment installed on your machine.</li> <li>Access to the GitHub repository for kube-burner: https://github.com/kube-burner/kube-burner.</li> </ol>"},{"location":"add-setup/05_kubeburner/#step-1-install-kubeburner","title":"Step 1: Install kubeburner","text":"<p>To begin, we need to download and install the kube-burner CLI. First, let's clone the repository from GitHub:</p> <pre><code>git clone https://github.com/kube-burner/kube-burner.git\ncd kube-burner\n</code></pre> <p>Now, build and install the binary for your operating system using the provided Makefile or by downloading a precompiled release:</p>"},{"location":"add-setup/05_kubeburner/#using-makefile-for-linux","title":"Using Makefile (for Linux)","text":"<pre><code>make\nsudo make install\n</code></pre>"},{"location":"add-setup/05_kubeburner/#or-downloading-a-precompiled-release","title":"Or downloading a precompiled release","text":"<pre><code>wget https://github.com/kube-burner/kube-burner/releases/download/v&lt;version&gt;/kube-burner-linux-amd64\nchmod +x kube-burner-linux-amd64\nsudo mv kube-burner-linux-amd64 /usr/local/bin/kube-burner\n</code></pre> <p>Replace  with the version number you'd like to download. <p>Step 2: Writing Templates for the Benchmark Application</p> <p>Now, we will create YAML templates for our benchmark application which includes a web server, a curl pod used to test it, and two network policies: deny-by-default and allow-{{.Replica}}-{{.Iteration}}.</p> <p>First, let's create the web server and its corresponding service using the given YAML templates below:</p>"},{"location":"add-setup/05_kubeburner/#create-a-new-directory-for-your-project","title":"Create a new directory for your project","text":"<pre><code>mkdir benchmark &amp;&amp; cd benchmark\n</code></pre> <pre><code>cat &gt; ubi9-template.yaml &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubi9-{{.Replica}}-{{.Iteration}}\n  labels:\n    app: ubi9\n    replica: {{.Replica}}\n    iteration: {{.Iteration}}\nspec:\n  template:\n    path: templates/ubi9\n    values:\n      replica: {{.Replica}}\n      iteration: {{.Iteration}}\n  restartPolicy: Always\nEOF\n</code></pre> <pre><code>cat &gt; ubi9-service.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: ubi9-{{.Replica}}-{{.Iteration}}\nspec:\n  selector:\n    matchLabels:\n      app: ubi9\n      replica: {{.Replica}}\n      iteration: {{.Iteration}}\n  ports:\n    - protocol: TCP\n      name: 8000\n      port: 8000\nEOF\n</code></pre> <p>Next, create templates for the curl pod and network policies using the given YAML templates below:</p> <pre><code>cat &gt; curl-template.yaml &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: curl-{{.Replica}}-{{.Iteration}}\n  labels:\n    app: curl\n    replica: {{.Replica}}\n    iteration: {{.Iteration}}\nspec:\n  template:\n    path: templates/curl\n    values:\n      replica: {{.Replica}}\n      iteration: {{.Iteration}}\n  restartPolicy: Always\nEOF\n</code></pre> <pre><code>cat &gt; deny-by-default-template.yaml &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-by-default\nspec:\n  podSelector: {}\n    ingress: []\nEOF\n</code></pre> <pre><code>cat &gt; allow-{{.Replica}}-{{.Iteration}}-template.yaml &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-{{.Replica}}-{{.Iteration}}\nspec:\n  podSelector:\n    matchLabels:\n      app: ubi9\n      replica: {{.Replica}}\n      iteration: {{.Iteration}}\n  ingress:\n  - ports:\n    - protocol: TCP\n      port: 8000\nEOF\n</code></pre>"},{"location":"add-setup/05_kubeburner/#step-3-install-elastic-kibana-and-grafafa","title":"Step 3: Install Elastic, Kibana and Grafafa","text":"<p>To be able to monitor our benchmark we gonna install some operator. Go in the Openshift Console and Install Elasticsearch (ECK) Operator and Grafana Operator.</p> <p>Then write the below configuration for the Operator</p> <pre><code>cat &gt; elastic.yaml &lt;&lt;EOF\napiVersion: elasticsearch.k8s.elastic.co/v1\nkind: Elasticsearch\nmetadata:\n  name: elasticsearch-kube-burner\n  namespace: kube-burner\nspec:\n  auth: {}\n  http:\n    service:\n      metadata: {}\n      spec: {}\n    tls:\n      certificate: {}\n  image: registry.access.redhat.com/elastic/elasticsearch\n  monitoring:\n    logs: {}\n    metrics: {}\n  nodeSets:\n    - config:\n        node.attr.attr_name: attr_value\n        node.roles:\n          - master\n          - data\n        node.store.allow_mmap: false\n      count: 3\n      name: default\n      podTemplate:\n        metadata:\n          creationTimestamp: null\n          labels:\n            foo: bar\n        spec:\n          containers:\n            - name: elasticsearch\n              resources:\n                limits:\n                  cpu: '2'\n                  memory: 4Gi\n                requests:\n                  cpu: '1'\n                  memory: 4Gi\n  transport:\n    service:\n      metadata: {}\n      spec: {}\n    tls:\n      certificate: {}\n      certificateAuthorities: {}\n  updateStrategy:\n    changeBudget: {}\n  version: 8.11.0\nEOF\n</code></pre> <pre><code>cat &gt; kibana.yaml &lt;&lt;EOF\napiVersion: kibana.k8s.elastic.co/v1\nkind: Kibana\nmetadata:\n  name: kibana-kube-burner\n  namespace: kube-burner\nspec:\n  count: 1\n  elasticsearchRef:\n    name: elasticsearch-sample\n  enterpriseSearchRef: {}\n  http:\n    service:\n      metadata: {}\n      spec: {}\n    tls:\n      certificate: {}\n  image: registry.access.redhat.com/elastic/kibana\n  monitoring:\n    logs: {}\n    metrics: {}\n  podTemplate:\n    metadata:\n      creationTimestamp: null\n      labels:\n        foo: bar\n    spec:\n      containers:\n        - name: kibana\n          resources:\n            limits:\n              cpu: '2'\n              memory: 2Gi\n            requests:\n              cpu: 500m\n              memory: 1Gi\n  version: 8.11.0\nEOF\n</code></pre> <pre><code>cat &gt; grafana.yaml &lt;&lt;EOF\napiVersion: integreatly.org/v1alpha1\nkind: Grafana\nmetadata:\n  name: grafana-v8\n  namespace: kube-burner\nspec:\n  baseImage: registry.access.redhat.com/rhel9/grafana\n  config:\n    auth.anonymous:\n      enabled: true\n    log:\n      level: warn\n      mode: console\n    security:\n      admin_password: hello\n      admin_user: admin\n  ingress:\n    enabled: true\nEOF\n</code></pre> <pre><code>oc apply -f elastic.yaml -f kibana.yaml -f grafana.yaml\n</code></pre> <p>Then save the elasticsearch password and route with the below command.</p> <pre><code>oc get secret elasticsearch-kube-burner-es-elastic-user -n kube-burner \\\n   -o go-template --template=\"{{.data.elastic|base64decode}}\"\n</code></pre> <pre><code>oc get route -n kube-burner\n</code></pre> <p>Finally we will create an ElasticSearchDataSource that will be used to plot the content of the genereted metrics from the index.</p> <pre><code>cat &gt; elasticGrafanaDataSource.yaml &lt;&lt;EOF\napiVersion: integreatly.org/v1alpha1\nkind: GrafanaDataSource\nmetadata:\n  name: elastic-kube-burner\nspec:\n  datasources:\n  - access: proxy\n    basicAuth: true\n    basicAuthPassword: YOUR_PASSWORD\n    basicAuthUser: elastic\n    database: kube-burner\n    editable: true\n    isDefault: false\n    jsonData:\n      tlsSkipVerify: true\n      esVersion: 70\n      timeField: timestamp\n      timeInterval: 5s\n    name: Elastic-kube-burner\n    type: elasticsearch\n    url: https://YOUR_ROUTE\n    version: 1\n  name: elastic-kube-burner.yaml\nEOF\n</code></pre> <pre><code>oc apply -f elasticGrafanaDataSource.yaml\n</code></pre>"},{"location":"add-setup/05_kubeburner/#step-4-using-kube-burner-to-deploy-the-benchmark-application","title":"Step 4: Using kube-burner to Deploy the Benchmark Application","text":"<p>Now that we have our YAML templates, let's use kube-burner to create and apply them on OpenShift. Create a Kube-Burner configuration file to deploy the benchmark application using Kube-Burner. Let's start by defining the basic structure of our configuration file.</p> <p>NOTE: Update the password and the route of elasticsearch with the one that you get in the previous step.</p> <pre><code>---\nglobal:\n  indexerConfig:\n    esServers: [https://elastic:YOURPASSWORD@YOUR_ES_ROUTE]\n    insecureSkipVerify: true\n    defaultIndex: kube-burner\n    type: elastic\n  measurements:\n    - name: podLatency\njobs:\n  - name: deny-all-policy\n    jobIterations: 1\n    qps: 1\n    burst: 1\n    namespacedIterations: false\n    namespace: kubelet-density-cni-networkpolicy\n    jobPause: 1m\n    objects:\n\n      - objectTemplate: templates/deny-all.yml\n        replicas: 1\n\n  - name: kubelet-density-cni-networkpolicy\n    jobIterations: 2\n    qps: 25\n    burst: 25\n    namespacedIterations: false\n    namespace: kubelet-density-cni-networkpolicy\n    waitWhenFinished: true\n    podWait: false\n    preLoadImages: true\n    preLoadPeriod: 2m\n    objects:\n\n      - objectTemplate: templates/ubi9-deployment.yaml\n        replicas: 1\n\n      - objectTemplate: templates/ubi9-service.yaml\n        replicas: 1\n\n      - objectTemplate: templates/allow-http.yml\n        replicas: 1\n\n      - objectTemplate: templates/curl-deployment.yaml\n        replicas: 1\n</code></pre> <p>Here's an explanation of the different parts:</p> <p>Global: This section sets some general configuration parameters for the kube-burner tool. Here, we configure the ElasticSearch server endpoint, disable SSL certificate verification, set the default index name, and specify that we will use ElasticSearch as our type of metrics provider.</p> <p>jobs: The jobs section contains a list of job definitions that represent the different workloads and tests we want to run. Each job has several attributes:</p> <ul> <li>name: A descriptive name for the job</li> <li>jobIterations: The number of times each test is repeated (useful when testing for stability)</li> <li>qps: The number of queries per second during a test's active phase</li> <li>burst: The maximum number of queries per second that can be sent during a test's bursting phase</li> <li>namespacedIterations: If true, each test will run in a separate Kubernetes namespace for better isolation</li> <li>namespace: The namespace where the test workloads and resources will be deployed</li> <li>jobPause: The number of seconds between job iterations (useful when testing long-running services)</li> </ul> <p>objects: The objects section lists the resource templates that kube-burner will create for each job. For example, one job might deploy a deployment, another might deploy a service, and yet another might create a network policy. The number of replicas for each template is specified in the \"replicas\" field.</p> <p>In summary, this configuration file sets up several jobs with varying workloads (e.g., a 'deny-all-policy' job, which deploys a network policy denying all traffic) and resources (such as deployments and services), and specifies the settings for each test (like the number of iterations, QPS, and bursts). The kube-burner tool will then use this configuration file to create, deploy, and manage these resources in a Kubernetes cluster.</p>"},{"location":"add-setup/05_kubeburner/#step-5-configure-metrics-file","title":"Step 5: Configure metrics file","text":"<p>Now we will configure a metrics file that will be used to collect metrics from prometheus to collect informations about the state of the cluster during our benchmark/test.</p> <p>```shell metrics.yaml cat &gt; elasticGrafanaDataSource.yaml &lt;&lt;EOF</p>"},{"location":"add-setup/05_kubeburner/#api-server","title":"API server","text":""},{"location":"add-setup/05_kubeburner/#api-server_1","title":"API server","text":"<ul> <li> <p>query: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver=\"kube-apiserver\", verb!~\"WATCH\", subresource!=\"log\"}[2m])) by (verb,resource,subresource,instance,le)) &gt; 0   metricName: API99thLatency</p> </li> <li> <p>query: sum(irate(apiserver_request_total{apiserver=\"kube-apiserver\",verb!=\"WATCH\",subresource!=\"log\"}[2m])) by (verb,instance,resource,code) &gt; 0   metricName: APIRequestRate</p> </li> <li> <p>query: sum(apiserver_current_inflight_requests{}) by (request_kind) &gt; 0   metricName: APIInflightRequests</p> </li> </ul>"},{"location":"add-setup/05_kubeburner/#containers-pod-metrics","title":"Containers &amp; pod metrics","text":"<ul> <li> <p>query: sum(irate(container_cpu_usage_seconds_total{name!=\"\",namespace=~\"openshift-(etcd|oauth-apiserver|.apiserver|ovn-kubernetes|sdn|ingress|authentication|.controller-manager|.*scheduler|monitoring|logging|image-registry)\"}[2m]) * 100) by (pod, namespace, node)   metricName: podCPU</p> </li> <li> <p>query: sum(container_memory_rss{name!=\"\",namespace=~\"openshift-(etcd|oauth-apiserver|.apiserver|ovn-kubernetes|sdn|ingress|authentication|.controller-manager|.*scheduler|monitoring|logging|image-registry)\"}) by (pod, namespace, node)   metricName: podMemory</p> </li> <li> <p>query: (sum(rate(container_fs_writes_bytes_total{container!=\"\",device!~\".+dm.+\"}[5m])) by (device, container, node) and on (node) kube_node_role{role=\"master\"}) &gt; 0   metricName: containerDiskUsage</p> </li> </ul>"},{"location":"add-setup/05_kubeburner/#kubelet-cri-o-metrics","title":"Kubelet &amp; CRI-O metrics","text":"<ul> <li> <p>query: sum(irate(process_cpu_seconds_total{service=\"kubelet\",job=\"kubelet\"}[2m]) * 100) by (node) and on (node) kube_node_role{role=\"worker\"}   metricName: kubeletCPU</p> </li> <li> <p>query: sum(process_resident_memory_bytes{service=\"kubelet\",job=\"kubelet\"}) by (node) and on (node) kube_node_role{role=\"worker\"}   metricName: kubeletMemory</p> </li> <li> <p>query: sum(irate(process_cpu_seconds_total{service=\"kubelet\",job=\"crio\"}[2m]) * 100) by (node) and on (node) kube_node_role{role=\"worker\"}   metricName: crioCPU</p> </li> <li> <p>query: sum(process_resident_memory_bytes{service=\"kubelet\",job=\"crio\"}) by (node) and on (node) kube_node_role{role=\"worker\"}   metricName: crioMemory</p> </li> </ul>"},{"location":"add-setup/05_kubeburner/#node-metrics","title":"Node metrics","text":"<ul> <li> <p>query: sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) &gt; 0   metricName: nodeCPU</p> </li> <li> <p>query: avg(node_memory_MemAvailable_bytes) by (instance)   metricName: nodeMemoryAvailable</p> </li> <li> <p>query: avg(node_memory_Active_bytes) by (instance)   metricName: nodeMemoryActive</p> </li> <li> <p>query: avg(node_memory_Cached_bytes) by (instance) + avg(node_memory_Buffers_bytes) by (instance)   metricName: nodeMemoryCached+nodeMemoryBuffers</p> </li> <li> <p>query: irate(node_network_receive_bytes_total{device=~\"^(ens|eth|bond|team).*\"}[2m])   metricName: rxNetworkBytes</p> </li> <li> <p>query: irate(node_network_transmit_bytes_total{device=~\"^(ens|eth|bond|team).*\"}[2m])   metricName: txNetworkBytes</p> </li> <li> <p>query: rate(node_disk_written_bytes_total{device!~\"^(dm|rb).*\"}[2m])   metricName: nodeDiskWrittenBytes</p> </li> <li> <p>query: rate(node_disk_read_bytes_total{device!~\"^(dm|rb).*\"}[2m])   metricName: nodeDiskReadBytes</p> </li> <li> <p>query: sum(rate(etcd_server_leader_changes_seen_total[2m]))   metricName: etcdLeaderChangesRate</p> </li> </ul>"},{"location":"add-setup/05_kubeburner/#etcd-metrics","title":"Etcd metrics","text":"<ul> <li> <p>query: etcd_server_is_leader &gt; 0   metricName: etcdServerIsLeader</p> </li> <li> <p>query: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[2m]))   metricName: 99thEtcdDiskBackendCommitDurationSeconds</p> </li> <li> <p>query: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))   metricName: 99thEtcdDiskWalFsyncDurationSeconds</p> </li> <li> <p>query: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))   metricName: 99thEtcdRoundTripTimeSeconds</p> </li> <li> <p>query: etcd_mvcc_db_total_size_in_bytes   metricName: etcdDBPhysicalSizeBytes</p> </li> <li> <p>query: etcd_mvcc_db_total_size_in_use_in_bytes   metricName: etcdDBLogicalSizeBytes</p> </li> <li> <p>query: sum(rate(etcd_object_counts{}[5m])) by (resource) &gt; 0   metricName: etcdObjectCount</p> </li> <li> <p>query: sum by (cluster_version)(etcd_cluster_version)   metricName: etcdVersion   instant: true</p> </li> </ul>"},{"location":"add-setup/05_kubeburner/#cluster-metrics","title":"Cluster metrics","text":"<ul> <li> <p>query: sum(kube_namespace_status_phase) by (phase) &gt; 0   metricName: namespaceCount</p> </li> <li> <p>query: sum(kube_pod_status_phase{}) by (phase)   metricName: podStatusCount</p> </li> <li> <p>query: count(kube_secret_info{})   metricName: secretCount</p> </li> <li> <p>query: count(kube_deployment_labels{})   metricName: deploymentCount</p> </li> <li> <p>query: count(kube_configmap_info{})   metricName: configmapCount</p> </li> <li> <p>query: count(kube_service_info{})   metricName: serviceCount</p> </li> <li> <p>query: count(openshift_route_created{})   metricName: routeCount   instant: true</p> </li> <li> <p>query: kube_node_role   metricName: nodeRoles   instant: true</p> </li> <li> <p>query: sum(kube_node_status_condition{status=\"true\"}) by (condition)   metricName: nodeStatus</p> </li> <li> <p>query: cluster_version{type=\"completed\"}   metricName: clusterVersion   instant: true EOF</p> </li> </ul> <pre><code>\nNow launch your benchmark with the below command :\n\n```shell\nkube-burner init -m ./metrics.yaml -c ./network-policy-density.yaml -u https://$(oc get route prometheus-k8s -n openshift-monitoring -o jsonpath=\"{.spec.host}\") --log-level=debug --token=$(oc create token prometheus-k8s -n openshift-monitoring)\n</code></pre>"},{"location":"add-setup/05_kubeburner/#validation","title":"Validation.","text":"<p>TODO present the namespace kubelet-density-cni-networkpolicy.</p> <p>To plot the metrics go in grafana and login with user <code>admin</code> and password <code>hello</code></p> <pre><code>oc get route grafana-route -n kube-burner\n</code></pre> <p>Go in dashboard and click on import and upload the below json file :</p> <pre><code>https://github.com/kube-burner/kube-burner/blob/main/examples/grafana-dashboards/kube-burner-latest.json\n</code></pre>"},{"location":"openshift/","title":"Introduction","text":""},{"location":"openshift/#introduction_1","title":"Introduction","text":"<p>La gestion efficace des appareils \u00e0 la p\u00e9riph\u00e9rie (edge) des r\u00e9seaux informatiques est devenue essentielle. Red Hat Device Edge \u00e9merge comme une solution polyvalente pour r\u00e9pondre aux d\u00e9fis sp\u00e9cifiques pos\u00e9s par la gestion des charges de travail sur des appareils \u00e0 la limite du r\u00e9seau. Cette plateforme offre une approche novatrice pour soutenir des t\u00e2ches vari\u00e9es sur des dispositifs aux ressources limit\u00e9es, rencontrant des contraintes de puissance, de refroidissement et de connectivit\u00e9.</p>"},{"location":"openshift/#quest-ce-que-red-hat-device-edge","title":"Qu'est-ce que Red Hat Device Edge ?","text":"<p>Red Hat Device Edge est une plateforme flexible con\u00e7ue pour r\u00e9pondre aux besoins des dispositifs \u00e0 la p\u00e9riph\u00e9rie, tels que les outils de ligne d'assemblage, les passerelles IoT, les points de vente et les contr\u00f4leurs industriels. Ces appareils op\u00e8rent souvent dans des environnements o\u00f9 les ressources informatiques sont limit\u00e9es et o\u00f9 l'acc\u00e8s peut \u00eatre restreint. Red Hat Device Edge offre une approche uniforme et adaptable pour prendre en charge diverses charges de travail sur ces appareils, malgr\u00e9 leurs contraintes en termes de ressources.</p> <p></p>"},{"location":"openshift/01_rhel-installation/","title":"Installation de Red Hat Enterprise Linux sur Scaleway","text":""},{"location":"openshift/01_rhel-installation/#telechargement-de-limage-qcow2","title":"T\u00e9l\u00e9chargement de l'image qcow2","text":"<p>Pour commencer, t\u00e9l\u00e9chargez l'image qcow2 de Red Hat Enterprise Linux depuis le lien suivant :  RHEL qcow image</p>"},{"location":"openshift/01_rhel-installation/#configuration-de-limage","title":"Configuration de l'image","text":"<p>Une fois t\u00e9l\u00e9charg\u00e9e, vous devez configurer l'image en changeant le mot de passe et en d\u00e9sactivant cloud-init \u00e0 l'aide de la commande suivante :</p> <pre><code>virt-customize -a ~/Downloads/rhel-9.3-x86_64-kvm.qcow2 --root-password password:yourpassword --uninstall cloud-init\n</code></pre>"},{"location":"openshift/01_rhel-installation/#importation-de-limage-sur-scaleway","title":"Importation de l'image sur Scaleway","text":"<ul> <li>Acc\u00e9dez \u00e0 la section \"Object Storage\" sur Scaleway.</li> <li>Cr\u00e9ez un bucket en sp\u00e9cifiant son nom et en choisissant la localisation (par exemple, Paris). </li> <li>T\u00e9l\u00e9versez l'image rhel-9.3-x86_64-kvm.qcow2 dans ce bucket nouvellement cr\u00e9\u00e9.</li> <li>Cliquez sur les trois points (...) \u00e0 c\u00f4t\u00e9 du fichier et s\u00e9lectionnez \"Import as a snapshot\". Choisissez le type \"Block SSD\" et importez le fichier en tant que snapshot. </li> <li>Acc\u00e9dez \u00e0 la section \"Instances\" sur Scaleway.</li> <li>Cliquez sur \"Create instance\".</li> <li>Une instance PLAY2_NANO suffit pour microshift. Dans la section \"Choose image\", acc\u00e9dez \u00e0 \"My snapshot\" et s\u00e9lectionnez le snapshot que vous venez d'importer. </li> <li>Configurez les d\u00e9tails de votre instance et proc\u00e9dez \u00e0 sa cr\u00e9ation.</li> </ul> <p>Une fois l'instance cr\u00e9\u00e9e, vous pourrez vous connecter depuis la console scaleway en utilisant les identifiants par d\u00e9faut : root/yourpassword. </p> <p>Cr\u00e9ez un nouvel utilisateur. </p> <pre><code>sudo useradd feven\n</code></pre> <pre><code>sudo passwd feven\n</code></pre> <pre><code>sudo usermod -aG wheel feven\n</code></pre> <p>Validez la connexion en vous connectant \u00e0 la machine via ssh</p> <pre><code>ssh feven@YourIP\n</code></pre> <p>Pour terminer, inscrivez-vous pour votre abonnement Red Hat</p> <pre><code>subscription-manager register\n</code></pre>"},{"location":"openshift/02_microshift-installation/","title":"Installation de MicroShift (mode RPM)","text":""},{"location":"openshift/02_microshift-installation/#procedure-dinstallation","title":"Proc\u00e9dure d'installation","text":""},{"location":"openshift/02_microshift-installation/#activation-des-depots-red-hat-de-microshift","title":"Activation des d\u00e9p\u00f4ts Red Hat de MicroShift","text":"<p>En tant qu'utilisateur root, ex\u00e9cutez la commande suivante pour activer les d\u00e9p\u00f4ts Red Hat de MicroShift :</p> <pre><code>sudo subscription-manager repos \\\n--enable rhocp-4.14-for-rhel-9-$(uname -i)-rpms \\\n--enable fast-datapath-for-rhel-9-$(uname -i)-rpms\n</code></pre> <p>Puis installez microshift</p> <pre><code>sudo dnf install -y microshift\n</code></pre>"},{"location":"openshift/02_microshift-installation/#copie-du-pull-secret-vers-etccrio","title":"Copie du pull secret vers /etc/crio","text":"<p>T\u00e9l\u00e9chargez votre pull secret d'installation depuis la Console Cloud Hybride Red Hat (https://console.redhat.com/openshift/install/pull-secret) vers un dossier temporaire, par exemple, $HOME/openshift-pull-secret. </p> <p>Pour copier le pull secret dans le dossier /etc/crio de votre machine RHEL, ex\u00e9cutez :</p> <pre><code>sudo cp $HOME/openshift-pull-secret /etc/crio/openshift-pull-secret\n</code></pre> <p>Rendre le fichier /etc/crio/openshift-pull-secret lisible et inscriptible par l'utilisateur root uniquement en en ex\u00e9cutant la commande suivante\u00a0:</p> <pre><code>sudo chown root:root /etc/crio/openshift-pull-secret\n</code></pre>"},{"location":"openshift/02_microshift-installation/#demarrez-red-hat-device-edge","title":"D\u00e9marrez Red Hat Device Edge.","text":"<p>En tant qu'utilisateur root, d\u00e9marrez MicroShift:</p> <pre><code>sudo systemctl start microshift\n</code></pre> <p>Pour configurer votre machine RHEL afin qu'elle d\u00e9marre MicroShift lorsque votre Machine d\u00e9marre</p> <pre><code>sudo systemctl enable microshift\n</code></pre>"},{"location":"openshift/03_microshift-connection/","title":"Connexion sur MicroShift","text":""},{"location":"openshift/03_microshift-connection/#acceder-localement-a-microshift","title":"Acc\u00e9der localement \u00e0 MicroShift","text":"<p>Copiez le fichier kubeconfig d'acc\u00e8s local g\u00e9n\u00e9r\u00e9 dans le r\u00e9pertoire ~/.kube/ en ex\u00e9cutant la commande commande suivante :</p> <pre><code>mkdir -p ~/.kube/\nsudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig &gt; ~/.kube/config\n</code></pre>"},{"location":"openshift/03_microshift-connection/#verification","title":"V\u00e9rification","text":"<p>Vous devriez maintenant avoir acc\u00e8s \u00e0 l'API. Pour v\u00e9rifier cela :</p> <pre><code>oc get no\n</code></pre> <pre><code>[feven@scw-friendly-austin ~]$ oc get no\nNAME                  STATUS   ROLES                         AGE   VERSION\nscw-friendly-austin   Ready    control-plane,master,worker   32m   v1.27.6\n</code></pre>"},{"location":"openshift/03_microshift-connection/#acceder-a-microshift-en-remote","title":"Acc\u00e9der \u00e0 MicroShift en remote","text":"<p>Pour cela nous allons devoir utiliser un nouvea kubconfig. </p> <p>Ouvrez/Creez le fichier suivant</p> <pre><code>vi /etc/microshift/config.yaml\n</code></pre> <p>Et ajouter la configuration ci-dessous en remplacent l'address IP, par l'address IP public de la machine</p> <pre><code>dns:\n  baseDomain: neutron-it.fr\nnode:\n  hostnameOverride: \"scw-friendly-austin\"\n  nodeIP: 10.75.52.217\napiServer:\n  subjectAltNames:\n  - YOURPUBLICIPADDRESS\n</code></pre> <p>Puis restart</p> <pre><code>systemctl restart microshift\n</code></pre> <p>Copiez le fichier kubeconfig d'acc\u00e8s remote g\u00e9n\u00e9r\u00e9 dans le r\u00e9pertoire </p> <pre><code>/var/lib/microshift/resources/kubeadmin/YOURPUBLICIPADDRESS/kubeconfig \n</code></pre>"},{"location":"openshift/04_add-console/","title":"Connexion a la console","text":""},{"location":"openshift/04_add-console/#deployer-la-console","title":"Deployer la console","text":"<p>Nous allons d\u00e9ployer la console OKD dans notre environnement. Pour contourner la n\u00e9cessit\u00e9 d'avoir un DNS configur\u00e9, nous allons utiliser nip.io.  </p> <p>NOTE: Avant de d\u00e9ployer le manifeste ci-dessous, modifiez l'adresse IP pour fournir celle de votre machine microshift.</p> <pre><code>cat &lt;&lt;EOF &gt; deploy_console.yaml\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ocp-console\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: openshift-console\n  namespace: ocp-console\nsecrets:\n- name: openshift-console-token\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-console-secret\n  namespace: ocp-console\n  annotations:\n    kubernetes.io/service-account.name: openshift-console\ntype: kubernetes.io/service-account-token\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: openshift-console-cluster-role-binding\n  namespace: ocp-console\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: openshift-console\n  namespace: ocp-console\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openshift-console-deployment\n  namespace: ocp-console\n  labels:\n    app: openshift-console\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-console\n  template:\n    metadata:\n      labels:\n        app: openshift-console\n    spec:\n      containers:\n      - name: openshift-console-app\n        image: quay.io/openshift/origin-console:4.8.0\n        env:\n        - name: BRIDGE_USER_AUTH\n          value: disabled\n        - name: BRIDGE_K8S_MODE\n          value: off-cluster\n        - name: BRIDGE_K8S_MODE_OFF_CLUSTER_ENDPOINT\n          value: https://api.51.15.55.253.nip.io:6443\n        - name: BRIDGE_K8S_MODE_OFF_CLUSTER_SKIP_VERIFY_TLS\n          value: \"true\"\n        - name: BRIDGE_K8S_AUTH\n          value: bearer-token\n        - name: BRIDGE_K8S_AUTH_BEARER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: openshift-console-secret\n              key: token\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: openshift-console-service\n  namespace: ocp-console\nspec:\n  selector:\n    app: openshift-console\n  ports:\n  - port: 80\n    targetPort: 9000\n---\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: openshift-console\n  namespace: ocp-console\nspec:\n  host: openshift-console.apps.51.15.55.253.nip.io\n  to:\n    kind: Service\n    name: openshift-console-service\n    weight: 100\n  port:\n    targetPort: 9000\n  wildcardPolicy: None\nEOF\n</code></pre> <pre><code>oc apply -f deploy_console.yaml\n</code></pre>"},{"location":"openshift/04_add-console/#verification","title":"V\u00e9rification","text":"<p>Vous pouvez maintenant acc\u00e9der \u00e0 la console OKD microshift </p> <p></p>"},{"location":"openshift/05_configure-storage/","title":"Configure dynamic storage","text":"<p>MicroShift permet un provisionnement de stockage dynamique et pr\u00eat \u00e0 l'emploi avec LVMS. Le plugin LVMS est la version Red Hat de TopoLVM, un plugin CSI permettant de g\u00e9rer les volumes LVM pour Kubernetes.</p>"},{"location":"openshift/05_configure-storage/#ajouter-du-stockage","title":"Ajouter du stockage","text":"<p>Dans un premier temps ajoutez un disque de stockage dans scaleway. Pour cela cliquez sur votre instance microshift puis sur Attached volumes et enfin sur Create volume. Selectionnez une taille (en GB) et cliquez sur Add Volume.</p> <p></p> <p>vous devrez ensuite identifier les disques que vous souhaitez utiliser pour le VG. </p> <pre><code>sudo pvcreate /dev/sdb\n</code></pre> <p>Une fois le volume physique pr\u00e9par\u00e9, vous pouvez cr\u00e9er un groupe de volumes nomm\u00e9 \u00ab microshift \u00bb et y ajouter le(s) volume(s) physique(s) initialis\u00e9(s)\u00a0:</p> <pre><code>sudo vgcreate microshift /dev/sdb\n</code></pre> <p>Redemarrez ensuite microshift </p> <pre><code>systemctl restart microshift\n</code></pre>"},{"location":"openshift/05_configure-storage/#verification","title":"V\u00e9rification","text":"<p>Vous devriez maintenant avoir un nouveau namespace \"openshift-storage\" dans la console. Avec un pod topolvm-controller et un pod topolvm-node Running.</p> <p></p> <p>You can now test by deploying an nginx basic application.</p> <pre><code>oc create ns demo\n</code></pre> <pre><code>kubectl config set-context --current --namespace=demo\n</code></pre> <pre><code>cat &lt;&lt;EOF | oc apply -f -\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: my-lv-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1G\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    command: [\"/usr/bin/sh\", \"-c\"]\n    args: [\"sleep 1h\"]\n    volumeMounts:\n    - mountPath: /mnt\n      name: my-volume\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n          - ALL\n      runAsNonRoot: true\n      seccompProfile:\n        type: RuntimeDefault\n  volumes:\n    - name: my-volume\n      persistentVolumeClaim:\n        claimName: my-lv-pvc\nEOF\n</code></pre>"}]}